---
draft: true
weight: 3
series: ["Matem√°tica para Machine Learning"]
series_order: 3
title: "C√°lculo Diferencial para Machine Learning: gradientes y optimizaci√≥n explicados para programadores"
description: "Descubre c√≥mo el c√°lculo diferencial impulsa todos los algoritmos de machine learning. Aprende gradient descent, derivadas parciales y optimizaci√≥n desde cero con ejemplos pr√°cticos en Python."
authors:
  - jnonino
date: 2025-09-09
tags: ["Inteligencia Artificial", "Aprendizaje Autom√°tico", "Machine Learning", "Matem√°tica", "C√°lculo Diferencia", "Gradiente", "Optimizaci√≥n"]
---
{{< katex >}}

# El Motor Invisible del Machine Learning: C√°lculo Diferencial

Imag√≠nate por un momento que ten√©s una pelota en la cima de una monta√±a irregular. Si la solt√°s, naturalmente va a rodar hacia abajo, buscando el punto m√°s bajo posible. **Esa pelota est√° resolviendo un problema de optimizaci√≥n usando las leyes de la f√≠sica.**

En el mundo del machine learning, nuestros algoritmos hacen exactamente lo mismo, pero en lugar de monta√±as f√≠sicas, navegan por paisajes matem√°ticos complejos llamados **funciones de costo**. Y la herramienta que les permite "saber hacia d√≥nde rodar" es el **c√°lculo diferencial**.

{{< alert >}}
**¬øPor qu√© necesit√°s entender c√°lculo para ML?** Porque literalmente **todos** los algoritmos de machine learning modernos (desde regresi√≥n lineal hasta transformers) usan gradient descent o variantes de optimizaci√≥n basadas en derivadas. Sin entender las derivadas, es como programar sin entender loops.
{{< /alert >}}

En las primeras dos semanas vimos los **qu√©** (fundamentos de IA) y los **d√≥nde** (√°lgebra lineal para representar datos). Esta semana vamos a ver el **c√≥mo**: **c√≥mo los algoritmos aprenden autom√°ticamente**.

## ¬øQu√© vas a aprender esta semana?

Al final de esta semana, vas a poder:

- **Entender intuitivamente** qu√© es una derivada y por qu√© es tan poderosa
- **Calcular gradientes** de funciones de m√∫ltiples variables
- **Implementar gradient descent desde cero** en Python
- **Visualizar** c√≥mo los algoritmos "aprenden" navegando funciones de costo
- **Optimizar** modelos de machine learning usando estos conceptos

Y lo m√°s importante: vas a **ver** el machine learning de una forma completamente nueva, entendiendo el motor matem√°tico que lo impulsa.

---

## Parte 1: El Concepto Fundamental - ¬øQu√© es una Derivada?

### La Intuici√≥n F√≠sica: Velocidad y Cambio

Antes de cualquier f√≥rmula, pensemos en t√©rminos f√≠sicos. Si est√°s manejando y quer√©s saber qu√© tan r√°pido est√°s acelerando, necesit√°s medir **c√≥mo cambia tu velocidad con respecto al tiempo**.

- Si tu velocidad pasa de 60 km/h a 80 km/h en 2 segundos, tu aceleraci√≥n promedio es: $\frac{80-60}{2-0} = 10$ km/h por segundo.
- Pero eso es el cambio **promedio**. ¬øQu√© pasa si quer√©s saber la aceleraci√≥n **exacta** en un momento espec√≠fico?

**Esa es exactamente la pregunta que responde la derivada.**

### Definici√≥n Matem√°tica (Sin Dolor)

Para cualquier funci√≥n $f(x)$, la derivada en un punto $x$ nos dice:

$$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$

**Traducido al espa√±ol**: "¬øQu√© tan r√°pido cambia $f(x)$ cuando $x$ cambia una cantidad infinitesimalmente peque√±a?"

Pero miremos esto visualmente:

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import pandas as pd

def visualizar_derivada_concepto():
    """
    Visualizaci√≥n interactiva del concepto de derivada
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # Funci√≥n ejemplo: f(x) = x^2
    x = np.linspace(-3, 3, 1000)
    y = x**2

    # Punto donde calculamos la derivada
    x_punto = 1
    y_punto = x_punto**2

    # Diferentes valores de h (incremento)
    h_values = [1, 0.5, 0.1, 0.01]
    colors = ['red', 'orange', 'blue', 'green']

    # Gr√°fico principal
    ax1.plot(x, y, 'k-', linewidth=2, label='f(x) = x¬≤')
    ax1.plot(x_punto, y_punto, 'ro', markersize=8, label=f'Punto ({x_punto}, {y_punto})')

    # Mostrar diferentes aproximaciones a la derivada
    for i, (h, color) in enumerate(zip(h_values, colors)):
        x_h = x_punto + h
        y_h = x_h**2

        # L√≠nea secante
        slope = (y_h - y_punto) / h
        x_line = np.array([x_punto - 0.5, x_punto + h + 0.5])
        y_line = y_punto + slope * (x_line - x_punto)

        ax1.plot(x_line, y_line, color=color, linestyle='--', alpha=0.7,
                label=f'h={h}, pendiente‚âà{slope:.2f}')
        ax1.plot(x_h, y_h, 'o', color=color, markersize=6)

    # Derivada exacta (pendiente verdadera)
    derivada_exacta = 2 * x_punto  # Para f(x) = x¬≤, f'(x) = 2x
    x_tangente = np.array([x_punto - 1, x_punto + 1])
    y_tangente = y_punto + derivada_exacta * (x_tangente - x_punto)
    ax1.plot(x_tangente, y_tangente, 'purple', linewidth=3,
            label=f'Tangente (derivada exacta = {derivada_exacta})')

    ax1.set_xlim(-2, 3)
    ax1.set_ylim(-1, 5)
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.set_title('Concepto Visual de la Derivada')
    ax1.set_xlabel('x')
    ax1.set_ylabel('f(x)')

    # Gr√°fico de convergencia
    h_range = np.logspace(-5, 0, 100)
    aproximaciones = [(((x_punto + h)**2) - y_punto) / h for h in h_range]

    ax2.semilogx(h_range, aproximaciones, 'b-', linewidth=2)
    ax2.axhline(y=derivada_exacta, color='purple', linestyle='--', linewidth=2,
               label=f'Derivada exacta = {derivada_exacta}')
    ax2.grid(True, alpha=0.3)
    ax2.set_title('Convergencia hacia la Derivada Exacta')
    ax2.set_xlabel('h (incremento)')
    ax2.set_ylabel('Aproximaci√≥n de la derivada')
    ax2.legend()

    plt.tight_layout()
    plt.show()

visualizar_derivada_concepto()
```

{{< alert "circle-info" >}}
**Insight clave**: Cuando $h$ se hace cada vez m√°s peque√±o, la pendiente de la l√≠nea secante se acerca cada vez m√°s a la pendiente de la l√≠nea tangente. **Esa pendiente de la tangente ES la derivada.**
{{< /alert >}}

### ¬øPor qu√© esto importa en Machine Learning?

En ML, siempre estamos tratando de **minimizar errores**. Imag√≠nate que ten√©s una funci√≥n que mide qu√© tan "mal" est√° tu modelo:

```python
def funcion_de_costo_simple(parametro):
    """
    Funci√≥n de costo hipot√©tica. En la vida real, esto podr√≠a ser
    el error cuadr√°tico medio de un modelo de regresi√≥n.
    """
    return (parametro - 3)**2 + 2

# Si sabemos que la derivada es 2*(parametro - 3),
# podemos encontrar hacia d√≥nde "mover" el par√°metro para reducir el error
def derivada_costo(parametro):
    return 2 * (parametro - 3)

# Ejemplo: si nuestro par√°metro actual es 5
parametro_actual = 5
print(f"Costo actual: {funcion_de_costo_simple(parametro_actual)}")
print(f"Derivada: {derivada_costo(parametro_actual)}")
print(f"Derivada > 0 significa: mover el par√°metro hacia la IZQUIERDA reduce el costo")
```

**La derivada nos dice exactamente en qu√© direcci√≥n cambiar nuestros par√°metros para mejorar el modelo.**

---

## Parte 2: Las Reglas del Juego - Derivaci√≥n B√°sica

### Reglas Fundamentales

Como programador, ya sab√©s que hay patrones y reglas que se repiten. En c√°lculo diferencial tambi√©n:

#### 1. Regla de la Potencia
Si $f(x) = x^n$, entonces $f'(x) = n \cdot x^{n-1}$

```python
def derivada_potencia(n, x):
    """Derivada de x^n"""
    if n == 0:
        return 0  # Derivada de constante
    return n * (x ** (n - 1))

# Ejemplos
print(f"Derivada de x¬≤ en x=3: {derivada_potencia(2, 3)}")  # 2*3 = 6
print(f"Derivada de x¬≥ en x=2: {derivada_potencia(3, 2)}")  # 3*4 = 12
```

#### 2. Regla de la Suma
Si $f(x) = g(x) + h(x)$, entonces $f'(x) = g'(x) + h'(x)$

**Esto es s√∫per importante**: la derivada es **lineal**. Pod√©s derivar cada t√©rmino por separado.

#### 3. Regla del Producto
Si $f(x) = g(x) \cdot h(x)$, entonces $f'(x) = g'(x) \cdot h(x) + g(x) \cdot h'(x)$

#### 4. Regla de la Cadena (La M√°s Importante para ML)
Si $f(x) = g(h(x))$, entonces $f'(x) = g'(h(x)) \cdot h'(x)$

**Esta es la regla que hace posible entrenar redes neuronales profundas** (backpropagation es b√°sicamente aplicar la regla de la cadena repetidamente).

### Derivadas de Funciones Comunes

Estas aparecen constantemente en ML:

| Funci√≥n | Derivada | Uso en ML |
|---------|----------|-----------|
| $x^n$ | $n \cdot x^{n-1}$ | Regresi√≥n polinomial |
| $e^x$ | $e^x$ | Funci√≥n exponencial |
| $\ln(x)$ | $\frac{1}{x}$ | Funci√≥n logar√≠tmica |
| $\sin(x)$ | $\cos(x)$ | An√°lisis de series temporales |
| $\frac{1}{1+e^{-x}}$ | $\frac{e^{-x}}{(1+e^{-x})^2}$ | **Funci√≥n sigmoid** |

```python
import numpy as np
import matplotlib.pyplot as plt

def mostrar_funciones_y_derivadas():
    """
    Visualizaci√≥n de funciones comunes y sus derivadas
    """
    x = np.linspace(-5, 5, 1000)

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()

    # 1. Funci√≥n cuadr√°tica
    y1 = x**2
    dy1 = 2*x
    axes[0].plot(x, y1, 'b-', label='f(x) = x¬≤', linewidth=2)
    axes[0].plot(x, dy1, 'r--', label="f'(x) = 2x", linewidth=2)
    axes[0].set_title('Funci√≥n Cuadr√°tica')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # 2. Funci√≥n exponencial
    y2 = np.exp(x)
    dy2 = np.exp(x)  # Su propia derivada!
    axes[1].plot(x, y2, 'b-', label='f(x) = eÀ£', linewidth=2)
    axes[1].plot(x, dy2, 'r--', label="f'(x) = eÀ£", linewidth=2)
    axes[1].set_title('Funci√≥n Exponencial')
    axes[1].set_ylim(0, 10)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    # 3. Funci√≥n sigmoid (s√∫per importante en ML)
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivada(x):
        s = sigmoid(x)
        return s * (1 - s)

    y3 = sigmoid(x)
    dy3 = sigmoid_derivada(x)
    axes[2].plot(x, y3, 'b-', label='œÉ(x) = 1/(1+e‚ÅªÀ£)', linewidth=2)
    axes[2].plot(x, dy3, 'r--', label="œÉ'(x) = œÉ(x)(1-œÉ(x))", linewidth=2)
    axes[2].set_title('Funci√≥n Sigmoid (Activaci√≥n Neural)')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)

    # 4. Funci√≥n logar√≠tmica
    x_pos = x[x > 0.1]  # Evitar log de n√∫meros negativos
    y4 = np.log(x_pos)
    dy4 = 1/x_pos
    axes[3].plot(x_pos, y4, 'b-', label='f(x) = ln(x)', linewidth=2)
    axes[3].plot(x_pos, dy4, 'r--', label="f'(x) = 1/x", linewidth=2)
    axes[3].set_title('Funci√≥n Logar√≠tmica')
    axes[3].set_xlim(0.1, 5)
    axes[3].legend()
    axes[3].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

mostrar_funciones_y_derivadas()
```

### Implementaci√≥n Pr√°ctica: Calculadora de Derivadas

Vamos a implementar un calculador de derivadas num√©rico y compararlo con el anal√≠tico:

```python
def derivada_numerica(func, x, h=1e-7):
    """
    Calcula la derivada num√©rica usando la definici√≥n de l√≠mite
    """
    return (func(x + h) - func(x)) / h

def comparar_derivadas():
    """
    Compara derivadas num√©ricas vs anal√≠ticas
    """
    # Funci√≥n test: f(x) = x¬≥ + 2x¬≤ - 5x + 1
    def f(x):
        return x**3 + 2*x**2 - 5*x + 1

    def f_derivada_analitica(x):
        # f'(x) = 3x¬≤ + 4x - 5
        return 3*x**2 + 4*x - 5

    puntos = np.linspace(-3, 3, 10)

    print("Comparaci√≥n Derivadas Num√©ricas vs Anal√≠ticas:")
    print("-" * 60)
    print(f"{'x':>8} {'Num√©rica':>12} {'Anal√≠tica':>12} {'Error':>12}")
    print("-" * 60)

    for x in puntos:
        numerica = derivada_numerica(f, x)
        analitica = f_derivada_analitica(x)
        error = abs(numerica - analitica)

        print(f"{x:>8.2f} {numerica:>12.6f} {analitica:>12.6f} {error:>12.2e}")

    # Visualizaci√≥n
    x_plot = np.linspace(-3, 3, 1000)
    y_original = [f(x) for x in x_plot]
    y_derivada_num = [derivada_numerica(f, x) for x in x_plot]
    y_derivada_ana = [f_derivada_analitica(x) for x in x_plot]

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    ax1.plot(x_plot, y_original, 'b-', linewidth=2, label='f(x) = x¬≥ + 2x¬≤ - 5x + 1')
    ax1.set_title('Funci√≥n Original')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    ax2.plot(x_plot, y_derivada_ana, 'g-', linewidth=2, label="Anal√≠tica: f'(x) = 3x¬≤ + 4x - 5")
    ax2.plot(x_plot, y_derivada_num, 'r--', linewidth=2, alpha=0.7, label='Num√©rica')
    ax2.set_title('Comparaci√≥n de Derivadas')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

comparar_derivadas()
```

{{< alert "lightbulb" >}}
**Insight pr√°ctico**: En machine learning, a menudo usamos derivadas num√©ricas para verificar que nuestras implementaciones de gradientes anal√≠ticos est√°n correctas. Esto se llama "gradient checking".
{{< /alert >}}

---

## Parte 3: El Mundo Multidimensional - Derivadas Parciales

### El Problema: Funciones de M√∫ltiples Variables

En machine learning, rara vez tenemos funciones de una sola variable. Un modelo t√≠pico podr√≠a tener millones de par√°metros:

```python
# En regresi√≥n lineal simple: y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b
# Necesitamos optimizar TODOS los par√°metros: w‚ÇÅ, w‚ÇÇ, ..., w‚Çô, b
```

¬øC√≥mo calculamos "la derivada" de una funci√≥n que depende de m√∫ltiples variables?

### Derivadas Parciales: Una Variable a la Vez

**Idea clave**: Mant√©n todas las variables constantes excepto una, y deriva con respecto a esa variable.

Para $f(x, y) = x^2 + 3xy + y^2$:

- $\frac{\partial f}{\partial x} = 2x + 3y$ (tratando $y$ como constante)
- $\frac{\partial f}{\partial y} = 3x + 2y$ (tratando $x$ como constante)

```python
def visualizar_derivadas_parciales():
    """
    Visualizaci√≥n 3D de derivadas parciales
    """
    fig = plt.figure(figsize=(18, 6))

    # Crear grilla de puntos
    x = np.linspace(-3, 3, 50)
    y = np.linspace(-3, 3, 50)
    X, Y = np.meshgrid(x, y)

    # Funci√≥n: f(x,y) = x¬≤ + 3xy + y¬≤
    Z = X**2 + 3*X*Y + Y**2

    # Derivadas parciales
    dZ_dx = 2*X + 3*Y  # ‚àÇf/‚àÇx
    dZ_dy = 3*X + 2*Y  # ‚àÇf/‚àÇy

    # Gr√°fico 1: Funci√≥n original
    ax1 = fig.add_subplot(131, projection='3d')
    surface = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)
    ax1.set_title('f(x,y) = x¬≤ + 3xy + y¬≤')
    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.set_zlabel('f(x,y)')

    # Gr√°fico 2: Derivada parcial respecto a x
    ax2 = fig.add_subplot(132, projection='3d')
    surface2 = ax2.plot_surface(X, Y, dZ_dx, cmap='Reds', alpha=0.8)
    ax2.set_title('‚àÇf/‚àÇx = 2x + 3y')
    ax2.set_xlabel('x')
    ax2.set_ylabel('y')
    ax2.set_zlabel('‚àÇf/‚àÇx')

    # Gr√°fico 3: Derivada parcial respecto a y
    ax3 = fig.add_subplot(133, projection='3d')
    surface3 = ax3.plot_surface(X, Y, dZ_dy, cmap='Blues', alpha=0.8)
    ax3.set_title('‚àÇf/‚àÇy = 3x + 2y')
    ax3.set_xlabel('x')
    ax3.set_ylabel('y')
    ax3.set_zlabel('‚àÇf/‚àÇy')

    plt.tight_layout()
    plt.show()

visualizar_derivadas_parciales()
```

### El Vector Gradiente: La Clave de Todo

El **gradiente** es simplemente el vector que contiene todas las derivadas parciales:

$$\nabla f(x, y) = \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{pmatrix}$$

**¬øPor qu√© es tan importante?** Porque el gradiente **siempre apunta en la direcci√≥n de m√°ximo crecimiento** de la funci√≥n.

```python
def visualizar_gradiente_direccion():
    """
    Visualizaci√≥n del gradiente como direcci√≥n de m√°ximo crecimiento
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

    # Funci√≥n simple para visualizar
    x = np.linspace(-3, 3, 100)
    y = np.linspace(-3, 3, 100)
    X, Y = np.meshgrid(x, y)
    Z = X**2 + Y**2  # Paraboloide simple

    # Gradiente: ‚àáf = [2x, 2y]
    dZ_dx = 2*X
    dZ_dy = 2*Y

    # Gr√°fico 1: Funci√≥n con curvas de nivel
    contour = ax1.contour(X, Y, Z, levels=20, colors='gray', alpha=0.5)
    ax1.clabel(contour, inline=True, fontsize=8)

    # Mostrar vectores gradiente en algunos puntos
    puntos_x = [-2, -1, 0, 1, 2]
    puntos_y = [-2, -1, 0, 1, 2]

    for px in puntos_x[::2]:  # Solo algunos puntos para claridad
        for py in puntos_y[::2]:
            if px != 0 or py != 0:  # Evitar el origen donde el gradiente es cero
                grad_x = 2*px
                grad_y = 2*py
                # Normalizar para mejor visualizaci√≥n
                norm = np.sqrt(grad_x**2 + grad_y**2)
                grad_x, grad_y = grad_x/norm * 0.3, grad_y/norm * 0.3

                ax1.arrow(px, py, grad_x, grad_y,
                         head_width=0.1, head_length=0.1,
                         fc='red', ec='red', alpha=0.8)

    ax1.set_title('Gradiente apunta hacia m√°ximo crecimiento')
    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.grid(True, alpha=0.3)
    ax1.set_aspect('equal')

    # Gr√°fico 2: Magnitud del gradiente
    grad_magnitude = np.sqrt(dZ_dx**2 + dZ_dy**2)
    im = ax2.contourf(X, Y, grad_magnitude, levels=20, cmap='hot')
    plt.colorbar(im, ax=ax2)
    ax2.set_title('Magnitud del Gradiente ||‚àáf||')
    ax2.set_xlabel('x')
    ax2.set_ylabel('y')

    plt.tight_layout()
    plt.show()

visualizar_gradiente_direccion()
```

{{< alert "circle-info" >}}
**Insight crucial**: Si quer√©s **minimizar** una funci√≥n, necesit√°s moverte en la direcci√≥n **opuesta** al gradiente. Eso es exactamente lo que hace gradient descent: **-‚àáf**.
{{< /alert >}}

### Implementaci√≥n de Gradientes Num√©ricos

```python
def gradiente_numerico(func, punto, h=1e-7):
    """
    Calcula el gradiente num√©rico de una funci√≥n en un punto

    Args:
        func: funci√≥n que toma un vector y devuelve un escalar
        punto: punto donde calcular el gradiente (array de numpy)
        h: paso para la aproximaci√≥n num√©rica

    Returns:
        gradiente: vector gradiente en el punto
    """
    punto = np.array(punto, dtype=float)
    grad = np.zeros_like(punto)

    for i in range(len(punto)):
        # Vector de perturbaci√≥n
        delta = np.zeros_like(punto)
        delta[i] = h

        # Aproximaci√≥n de derivada parcial
        grad[i] = (func(punto + delta) - func(punto - delta)) / (2 * h)

    return grad

# Ejemplo de uso
def funcion_ejemplo(params):
    """f(x,y) = x¬≤ + 3xy + y¬≤"""
    x, y = params
    return x**2 + 3*x*y + y**2

def gradiente_analitico_ejemplo(params):
    """Gradiente anal√≠tico de f(x,y) = x¬≤ + 3xy + y¬≤"""
    x, y = params
    return np.array([2*x + 3*y, 3*x + 2*y])

# Verificaci√≥n
punto_test = [1.5, -0.8]
grad_num = gradiente_numerico(funcion_ejemplo, punto_test)
grad_ana = gradiente_analitico_ejemplo(punto_test)

print("Verificaci√≥n de Gradientes:")
print(f"Punto: {punto_test}")
print(f"Gradiente num√©rico:  {grad_num}")
print(f"Gradiente anal√≠tico: {grad_ana}")
print(f"Error: {np.linalg.norm(grad_num - grad_ana)}")
```

---

## Parte 4: La Magia de la Optimizaci√≥n

### El Problema Fundamental del Machine Learning

Todo en machine learning se reduce a esto:

1. Defin√≠s una **funci√≥n de costo** $J(\theta)$ que mide qu√© tan "malo" es tu modelo
2. Encontr√°s los par√°metros $\theta$ que **minimizan** esa funci√≥n
3. Esos par√°metros √≥ptimos son tu "modelo entrenado"

El gradient descent es el algoritmo que resuelve el paso 2.

### Gradient Descent: El Algoritmo Estrella

**Idea s√∫per simple**:
1. Empez√°s con par√°metros aleatorios
2. Calcul√°s el gradiente (¬øen qu√© direcci√≥n crece la funci√≥n?)
3. Te mov√©s en la direcci√≥n **opuesta** (para minimizar)
4. Repet√≠s hasta converger

Matem√°ticamente:

$$\theta_{nuevo} = \theta_{actual} - \alpha \nabla J(\theta_{actual})$$

Donde $\alpha$ es el **learning rate** (qu√© tan grandes son los pasos).

```python
def gradient_descent_visual():
    """
    Visualizaci√≥n completa del algoritmo gradient descent
    """
    # Funci√≥n objetivo: f(x,y) = (x-2)¬≤ + (y+1)¬≤ + 3
    # M√≠nimo en (2, -1)
    def funcion_objetivo(params):
        x, y = params
        return (x - 2)**2 + (y + 1)**2 + 3

    def gradiente_funcion(params):
        x, y = params
        return np.array([2*(x - 2), 2*(y + 1)])

    # Configuraci√≥n
    learning_rates = [0.1, 0.3, 0.5, 0.9]
    punto_inicial = [-1, 2]
    max_iteraciones = 20

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()

    # Crear superficie para visualizaci√≥n
    x_range = np.linspace(-3, 4, 100)
    y_range = np.linspace(-3, 3, 100)
    X, Y = np.meshgrid(x_range, y_range)
    Z = (X - 2)**2 + (Y + 1)**2 + 3

    for idx, lr in enumerate(learning_rates):
        ax = axes[idx]

        # Contorno de la funci√≥n
        contour = ax.contour(X, Y, Z, levels=20, colors='gray', alpha=0.5)

        # Gradient descent
        trayectoria_x = [punto_inicial[0]]
        trayectoria_y = [punto_inicial[1]]
        costos = [funcion_objetivo(punto_inicial)]

        punto_actual = np.array(punto_inicial, dtype=float)

        for i in range(max_iteraciones):
            grad = gradiente_funcion(punto_actual)
            punto_actual = punto_actual - lr * grad

            trayectoria_x.append(punto_actual[0])
            trayectoria_y.append(punto_actual[1])
            costos.append(funcion_objetivo(punto_actual))

            # Si converge, parar
            if np.linalg.norm(grad) < 1e-6:
                break

        # Plotear trayectoria
        ax.plot(trayectoria_x, trayectoria_y, 'ro-', markersize=4, linewidth=2,
                label=f'Learning Rate = {lr}')
        ax.plot(punto_inicial[0], punto_inicial[1], 'go', markersize=10,
                label='Inicio')
        ax.plot(2, -1, 'r*', markersize=15, label='M√≠nimo Global')

        ax.set_title(f'LR = {lr}, Iteraciones: {len(trayectoria_x)-1}')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_xlim(-3, 4)
        ax.set_ylim(-3, 3)

    plt.tight_layout()
    plt.show()

    return learning_rates, costos

gradient_descent_visual()
```

### An√°lisis del Learning Rate

El **learning rate** es cr√≠tico. Muy peque√±o ‚Üí converges lento. Muy grande ‚Üí no converges nunca.

```python
def analisis_learning_rate():
    """
    An√°lisis detallado del impacto del learning rate
    """
    def f(x):
        return x**2 + 2  # Funci√≥n simple 1D

    def df_dx(x):
        return 2*x

    learning_rates = [0.1, 0.5, 1.0, 1.5]  # El √∫ltimo es demasiado grande
    x_inicial = 3.0
    iteraciones = 20

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Funci√≥n objetivo
    x_plot = np.linspace(-4, 4, 1000)
    y_plot = f(x_plot)

    for lr in learning_rates:
        trayectoria = [x_inicial]
        costos = [f(x_inicial)]
        x_actual = x_inicial

        for i in range(iteraciones):
            grad = df_dx(x_actual)
            x_nuevo = x_actual - lr * grad

            # Verificar divergencia
            if abs(x_nuevo) > 100:
                print(f"¬°Learning rate {lr} causa divergencia en iteraci√≥n {i}!")
                break

            trayectoria.append(x_nuevo)
            costos.append(f(x_nuevo))
            x_actual = x_nuevo

        # Gr√°fico 1: Trayectoria en el espacio de par√°metros
        ax1.plot(x_plot, y_plot, 'k-', alpha=0.3)
        if len(trayectoria) < 50:  # Solo plotear si no diverge
            ax1.plot(trayectoria, [f(x) for x in trayectoria], 'o-',
                    label=f'LR = {lr}', markersize=4)

    ax1.axhline(y=2, color='red', linestyle='--', label='M√≠nimo Global')
    ax1.set_xlabel('Par√°metro x')
    ax1.set_ylabel('Costo f(x)')
    ax1.set_title('Trayectorias de Gradient Descent')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(-4, 4)
    ax1.set_ylim(0, 20)

    # Gr√°fico 2: Convergencia del costo
    for lr in learning_rates:
        costos = [f(x_inicial)]
        x_actual = x_inicial

        for i in range(iteraciones):
            grad = df_dx(x_actual)
            x_actual = x_actual - lr * grad
            if abs(x_actual) > 100:
                break
            costos.append(f(x_actual))

        if len(costos) < 50:
            ax2.semilogy(range(len(costos)), costos, 'o-',
                        label=f'LR = {lr}', markersize=4)

    ax2.axhline(y=2, color='red', linestyle='--', label='M√≠nimo Global')
    ax2.set_xlabel('Iteraci√≥n')
    ax2.set_ylabel('Costo (escala log)')
    ax2.set_title('Convergencia del Costo')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

analisis_learning_rate()
```

{{< alert "warning" >}}
**Reglas pr√°cticas para Learning Rate**:
- Empez√° con 0.01 o 0.1
- Si el costo aumenta ‚Üí LR muy alto, reduc√≠ por 10
- Si converge muy lento ‚Üí LR muy bajo, aument√° por 2-5
- Us√° t√©cnicas como learning rate scheduling o algoritmos adaptativos
{{< /alert >}}

### Funciones de Costo en Machine Learning

Veamos las funciones que realmente optimizamos en ML:

```python
def funciones_costo_ml():
    """
    Visualizaci√≥n de funciones de costo comunes en ML
    """
    # Datos sint√©ticos para ejemplos
    np.random.seed(42)
    n_puntos = 100
    x_true = np.linspace(0, 1, n_puntos)
    y_true = 2 * x_true + 1 + 0.1 * np.random.randn(n_puntos)

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # 1. Error Cuadr√°tico Medio (MSE) - Regresi√≥n
    def mse(w, b):
        y_pred = w * x_true + b
        return np.mean((y_true - y_pred)**2)

    w_range = np.linspace(0, 4, 50)
    b_range = np.linspace(-1, 3, 50)
    W, B = np.meshgrid(w_range, b_range)
    MSE_surface = np.array([[mse(w, b) for w in w_range] for b in b_range])

    contour1 = axes[0,0].contour(W, B, MSE_surface, levels=20)
    axes[0,0].clabel(contour1, inline=True, fontsize=8)
    axes[0,0].plot(2, 1, 'r*', markersize=15, label='M√≠nimo verdadero')
    axes[0,0].set_title('MSE para Regresi√≥n Lineal')
    axes[0,0].set_xlabel('Peso (w)')
    axes[0,0].set_ylabel('Sesgo (b)')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)

    # 2. Funci√≥n de p√©rdida log√≠stica - Clasificaci√≥n
    # Simulamos datos de clasificaci√≥n
    y_class = (y_true > np.mean(y_true)).astype(int)

    def logistic_loss(w, b):
        z = w * x_true + b
        # Evitar overflow num√©rico
        z = np.clip(z, -500, 500)
        return np.mean(np.log(1 + np.exp(-y_class * z + (1-y_class) * z)))

    # Esta funci√≥n es m√°s compleja de visualizar, usamos una aproximaci√≥n
    w_range2 = np.linspace(-1, 5, 30)
    b_range2 = np.linspace(-2, 4, 30)
    W2, B2 = np.meshgrid(w_range2, b_range2)

    Logistic_surface = np.zeros_like(W2)
    for i in range(len(b_range2)):
        for j in range(len(w_range2)):
            try:
                Logistic_surface[i,j] = logistic_loss(W2[i,j], B2[i,j])
            except:
                Logistic_surface[i,j] = np.inf

    # Reemplazar infinitos con valores grandes
    Logistic_surface = np.where(np.isinf(Logistic_surface),
                               np.max(Logistic_surface[np.isfinite(Logistic_surface)]) * 2,
                               Logistic_surface)

    contour2 = axes[0,1].contour(W2, B2, Logistic_surface, levels=20)
    axes[0,1].set_title('P√©rdida Log√≠stica para Clasificaci√≥n')
    axes[0,1].set_xlabel('Peso (w)')
    axes[0,1].set_ylabel('Sesgo (b)')
    axes[0,1].grid(True, alpha=0.3)

    # 3. Funci√≥n de activaci√≥n y su derivada
    z = np.linspace(-6, 6, 1000)
    sigmoid = 1 / (1 + np.exp(-z))
    sigmoid_deriv = sigmoid * (1 - sigmoid)

    axes[1,0].plot(z, sigmoid, 'b-', linewidth=2, label='œÉ(z) = 1/(1+e^-z)')
    axes[1,0].plot(z, sigmoid_deriv, 'r--', linewidth=2, label="œÉ'(z) = œÉ(z)(1-œÉ(z))")
    axes[1,0].set_title('Funci√≥n Sigmoid y su Derivada')
    axes[1,0].set_xlabel('z')
    axes[1,0].legend()
    axes[1,0].grid(True, alpha=0.3)

    # 4. Comparaci√≥n de optimizadores
    def optimizar_comparacion():
        # Funci√≥n objetivo complicada con m√∫ltiples m√≠nimos locales
        def f_compleja(x, y):
            return 0.5 * (x**2 + y**2) + 0.3 * np.sin(5*x) + 0.3 * np.cos(5*y)

        def grad_f_compleja(x, y):
            dx = x + 1.5 * np.cos(5*x)
            dy = y - 1.5 * np.sin(5*y)
            return np.array([dx, dy])

        # SGD cl√°sico
        punto = np.array([2.0, 2.0])
        lr = 0.1
        trayectoria_sgd = [punto.copy()]

        for _ in range(100):
            grad = grad_f_compleja(punto[0], punto[1])
            punto = punto - lr * grad
            trayectoria_sgd.append(punto.copy())

        # SGD con momentum
        punto_mom = np.array([2.0, 2.0])
        velocidad = np.zeros(2)
        momentum = 0.9
        trayectoria_momentum = [punto_mom.copy()]

        for _ in range(100):
            grad = grad_f_compleja(punto_mom[0], punto_mom[1])
            velocidad = momentum * velocidad - lr * grad
            punto_mom = punto_mom + velocidad
            trayectoria_momentum.append(punto_mom.copy())

        # Visualizar
        x_range = np.linspace(-3, 3, 100)
        y_range = np.linspace(-3, 3, 100)
        X, Y = np.meshgrid(x_range, y_range)
        Z = f_compleja(X, Y)

        axes[1,1].contour(X, Y, Z, levels=30, colors='gray', alpha=0.5)

        traj_sgd = np.array(trayectoria_sgd)
        traj_mom = np.array(trayectoria_momentum)

        axes[1,1].plot(traj_sgd[:, 0], traj_sgd[:, 1], 'b-',
                      label='SGD cl√°sico', linewidth=2, alpha=0.7)
        axes[1,1].plot(traj_mom[:, 0], traj_mom[:, 1], 'r-',
                      label='SGD + Momentum', linewidth=2, alpha=0.7)
        axes[1,1].plot(2, 2, 'go', markersize=8, label='Inicio')

        axes[1,1].set_title('Comparaci√≥n de Optimizadores')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
        axes[1,1].set_xlim(-3, 3)
        axes[1,1].set_ylim(-3, 3)

    optimizar_comparacion()
    plt.tight_layout()
    plt.show()

funciones_costo_ml()
```

---

## Parte 5: Implementaci√≥n desde Cero

### Gradient Descent para Regresi√≥n Lineal

Vamos a implementar gradient descent completamente desde cero para resolver un problema real:

```python
class GradientDescentLinear:
    """
    Implementaci√≥n completa de Gradient Descent para Regresi√≥n Lineal
    """

    def __init__(self, learning_rate=0.01, max_iter=1000, tolerance=1e-6):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.tolerance = tolerance
        self.history = {'cost': [], 'weights': [], 'bias': []}

    def _add_intercept(self, X):
        """Agregar columna de 1s para el t√©rmino de sesgo"""
        intercept = np.ones((X.shape[0], 1))
        return np.concatenate((intercept, X), axis=1)

    def _cost_function(self, h, y):
        """Calcular funci√≥n de costo MSE"""
        return (1 / (2 * len(y))) * np.sum((h - y) ** 2)

    def _gradient(self, X, h, y):
        """Calcular gradiente de la funci√≥n de costo"""
        return (1 / len(y)) * X.T.dot(h - y)

    def fit(self, X, y, verbose=True):
        """
        Entrenar el modelo usando gradient descent
        """
        # Agregar t√©rmino de intercept
        X = self._add_intercept(X)

        # Inicializar par√°metros aleatoriamente
        self.theta = np.random.normal(0, 0.01, X.shape[1])

        if verbose:
            print(f"Iniciando entrenamiento con {len(X)} muestras...")
            print(f"Learning rate: {self.learning_rate}")
            print("-" * 50)

        for i in range(self.max_iter):
            # Forward pass: calcular predicciones
            h = X.dot(self.theta)

            # Calcular costo
            cost = self._cost_function(h, y)

            # Calcular gradiente
            gradient = self._gradient(X, h, y)

            # Actualizar par√°metros
            theta_anterior = self.theta.copy()
            self.theta -= self.learning_rate * gradient

            # Guardar historial
            self.history['cost'].append(cost)
            self.history['weights'].append(self.theta[1:].copy())
            self.history['bias'].append(self.theta[0])

            # Verificar convergencia
            if np.linalg.norm(self.theta - theta_anterior) < self.tolerance:
                if verbose:
                    print(f"Convergencia alcanzada en iteraci√≥n {i}")
                break

            if verbose and i % 100 == 0:
                print(f"Iteraci√≥n {i:4d}: Costo = {cost:.6f}")

        if verbose:
            print(f"Entrenamiento completado!")
            print(f"Par√°metros finales: {self.theta}")
            print(f"Costo final: {self.history['cost'][-1]:.6f}")

        return self

    def predict(self, X):
        """Hacer predicciones"""
        X = self._add_intercept(X)
        return X.dot(self.theta)

    def plot_training_history(self):
        """Visualizar el proceso de entrenamiento"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

        iterations = range(len(self.history['cost']))

        # 1. Convergencia del costo
        ax1.plot(iterations, self.history['cost'], 'b-', linewidth=2)
        ax1.set_title('Convergencia de la Funci√≥n de Costo')
        ax1.set_xlabel('Iteraci√≥n')
        ax1.set_ylabel('MSE')
        ax1.grid(True, alpha=0.3)
        ax1.semilogy()  # Escala logar√≠tmica para mejor visualizaci√≥n

        # 2. Evoluci√≥n de los pesos
        weights_history = np.array(self.history['weights'])
        if weights_history.shape[1] <= 5:  # Solo si no hay demasiados features
            for i in range(weights_history.shape[1]):
                ax2.plot(iterations, weights_history[:, i],
                        label=f'Peso {i+1}', linewidth=2)
        ax2.set_title('Evoluci√≥n de los Pesos')
        ax2.set_xlabel('Iteraci√≥n')
        ax2.set_ylabel('Valor del Peso')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. Evoluci√≥n del sesgo
        ax3.plot(iterations, self.history['bias'], 'g-', linewidth=2)
        ax3.set_title('Evoluci√≥n del Sesgo (Intercept)')
        ax3.set_xlabel('Iteraci√≥n')
        ax3.set_ylabel('Valor del Sesgo')
        ax3.grid(True, alpha=0.3)

        # 4. Gradiente de la funci√≥n de costo (aproximado por cambio en par√°metros)
        cambios_theta = [0] + [np.linalg.norm(np.array(self.history['weights'][i]) -
                                             np.array(self.history['weights'][i-1]))
                              for i in range(1, len(self.history['weights']))]
        ax4.semilogy(iterations, cambios_theta, 'r-', linewidth=2)
        ax4.set_title('Magnitud del Cambio en Par√°metros')
        ax4.set_xlabel('Iteraci√≥n')
        ax4.set_ylabel('||ŒîŒ∏|| (escala log)')
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

# Ejemplo de uso completo
def demo_gradient_descent():
    """
    Demostraci√≥n completa del algoritmo
    """
    print("üöÄ DEMO: Gradient Descent para Regresi√≥n Lineal")
    print("=" * 60)

    # 1. Generar datos sint√©ticos
    np.random.seed(42)
    n_samples = 200
    n_features = 2

    X = np.random.randn(n_samples, n_features)
    true_weights = np.array([3, -2])
    true_bias = 1.5
    noise = 0.1 * np.random.randn(n_samples)

    y = X.dot(true_weights) + true_bias + noise

    print(f"Datos generados:")
    print(f"  - {n_samples} muestras, {n_features} caracter√≠sticas")
    print(f"  - Pesos verdaderos: {true_weights}")
    print(f"  - Sesgo verdadero: {true_bias}")
    print()

    # 2. Entrenar modelo
    modelo = GradientDescentLinear(learning_rate=0.1, max_iter=2000)
    modelo.fit(X, y, verbose=True)

    print()
    print("üìä RESULTADOS:")
    print("-" * 30)
    print(f"Pesos estimados: {modelo.theta[1:]}")
    print(f"Sesgo estimado: {modelo.theta[0]:.4f}")
    print(f"Error en pesos: {np.abs(modelo.theta[1:] - true_weights)}")
    print(f"Error en sesgo: {abs(modelo.theta[0] - true_bias):.4f}")

    # 3. Evaluar modelo
    y_pred = modelo.predict(X)
    mse = np.mean((y - y_pred) ** 2)
    r2 = 1 - (np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2))

    print()
    print("üìà M√âTRICAS:")
    print("-" * 20)
    print(f"MSE: {mse:.4f}")
    print(f"R¬≤:  {r2:.4f}")

    # 4. Visualizaciones
    modelo.plot_training_history()

    # 5. Comparaci√≥n visual (solo para 1D)
    if n_features == 1:
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.scatter(X, y, alpha=0.6, label='Datos reales')
        plt.plot(X, y_pred, 'r-', linewidth=2, label='Predicciones')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.legend()
        plt.title('Ajuste del Modelo')
        plt.grid(True, alpha=0.3)

        plt.subplot(1, 2, 2)
        residuos = y - y_pred
        plt.scatter(y_pred, residuos, alpha=0.6)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('Predicciones')
        plt.ylabel('Residuos')
        plt.title('An√°lisis de Residuos')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    return modelo, X, y

# Ejecutar demo
modelo_entrenado, X_data, y_data = demo_gradient_descent()
```

### Comparaci√≥n con Soluci√≥n Anal√≠tica

Para verificar que nuestro gradient descent funciona, compar√©moslo con la soluci√≥n anal√≠tica de regresi√≥n lineal:

```python
def comparar_con_solucion_analitica():
    """
    Compara gradient descent con la soluci√≥n de forma cerrada
    """
    print("üîç COMPARACI√ìN: Gradient Descent vs Soluci√≥n Anal√≠tica")
    print("=" * 70)

    # Usar los mismos datos del ejemplo anterior
    X = X_data
    y = y_data

    # 1. Soluci√≥n anal√≠tica (Normal Equation)
    # Œ∏ = (X^T X)^(-1) X^T y
    X_with_intercept = np.column_stack([np.ones(len(X)), X])
    theta_analitica = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y

    print("Soluci√≥n Anal√≠tica:")
    print(f"  Sesgo: {theta_analitica[0]:.6f}")
    print(f"  Pesos: {theta_analitica[1:]}")

    print()
    print("Gradient Descent:")
    print(f"  Sesgo: {modelo_entrenado.theta[0]:.6f}")
    print(f"  Pesos: {modelo_entrenado.theta[1:]}")

    print()
    print("Diferencias:")
    diferencias = np.abs(modelo_entrenado.theta - theta_analitica)
    print(f"  Sesgo: {diferencias[0]:.8f}")
    print(f"  Pesos: {diferencias[1:]}")
    print(f"  Error total: {np.linalg.norm(diferencias):.8f}")

    # Verificar predicciones
    pred_analitica = X_with_intercept @ theta_analitica
    pred_gd = modelo_entrenado.predict(X)

    mse_analitica = np.mean((y - pred_analitica) ** 2)
    mse_gd = np.mean((y - pred_gd) ** 2)

    print()
    print("Comparaci√≥n de MSE:")
    print(f"  Soluci√≥n Anal√≠tica: {mse_analitica:.8f}")
    print(f"  Gradient Descent:   {mse_gd:.8f}")
    print(f"  Diferencia:         {abs(mse_analitica - mse_gd):.10f}")

comparar_con_solucion_analitica()
```

### Visualizaci√≥n 3D del Landscape de Optimizaci√≥n

Para realmente entender c√≥mo funciona gradient descent, veamos el "paisaje" que est√° navegando:

```python
def visualizar_landscape_3d():
    """
    Visualizaci√≥n 3D del proceso de optimizaci√≥n
    """
    # Usar un ejemplo 2D simple para poder visualizar el landscape completo
    # Datos sint√©ticos 1D
    np.random.seed(42)
    X_simple = np.linspace(0, 1, 20).reshape(-1, 1)
    y_simple = 2 * X_simple.flatten() + 1 + 0.1 * np.random.randn(20)

    # Funci√≥n de costo en funci√≥n de w (peso) y b (sesgo)
    def cost_surface(w, b):
        y_pred = w * X_simple.flatten() + b
        return np.mean((y_simple - y_pred) ** 2)

    # Crear grid para la superficie
    w_range = np.linspace(-1, 5, 50)
    b_range = np.linspace(-1, 4, 50)
    W, B = np.meshgrid(w_range, b_range)

    Cost = np.array([[cost_surface(w, b) for w in w_range] for b in b_range])

    # Ejecutar gradient descent y guardar trayectoria
    modelo_3d = GradientDescentLinear(learning_rate=0.1, max_iter=100)
    modelo_3d.fit(X_simple, y_simple, verbose=False)

    # Extraer trayectoria
    w_traj = [w[0] if len(w) > 0 else 0 for w in modelo_3d.history['weights']]
    b_traj = modelo_3d.history['bias']
    cost_traj = modelo_3d.history['cost']

    # Crear visualizaci√≥n 3D
    fig = plt.figure(figsize=(20, 6))

    # 1. Vista 3D del landscape
    ax1 = fig.add_subplot(131, projection='3d')
    surface = ax1.plot_surface(W, B, Cost, cmap='viridis', alpha=0.6)

    # Plotear trayectoria de gradient descent
    ax1.plot(w_traj, b_traj, cost_traj, 'ro-', linewidth=3, markersize=4,
             label='Trayectoria GD')
    ax1.plot(w_traj[0], b_traj[0], cost_traj[0], 'go', markersize=10,
             label='Inicio')
    ax1.plot(w_traj[-1], b_traj[-1], cost_traj[-1], 'r*', markersize=15,
             label='Final')

    ax1.set_xlabel('Peso (w)')
    ax1.set_ylabel('Sesgo (b)')
    ax1.set_zlabel('Costo MSE')
    ax1.set_title('Landscape 3D de Optimizaci√≥n')
    ax1.legend()

    # 2. Vista contorno desde arriba
    ax2 = fig.add_subplot(132)
    contour = ax2.contour(W, B, Cost, levels=30, colors='gray', alpha=0.6)
    ax2.clabel(contour, inline=True, fontsize=8)

    ax2.plot(w_traj, b_traj, 'ro-', linewidth=3, markersize=4)
    ax2.plot(w_traj[0], b_traj[0], 'go', markersize=10, label='Inicio')
    ax2.plot(w_traj[-1], b_traj[-1], 'r*', markersize=15, label='Final')

    # Agregar vectores de gradiente en algunos puntos
    for i in range(0, len(w_traj), 10):
        if i < len(w_traj) - 1:
            dw = w_traj[i+1] - w_traj[i]
            db = b_traj[i+1] - b_traj[i]
            ax2.arrow(w_traj[i], b_traj[i], dw*10, db*10,
                     head_width=0.05, head_length=0.05, fc='blue', ec='blue')

    ax2.set_xlabel('Peso (w)')
    ax2.set_ylabel('Sesgo (b)')
    ax2.set_title('Vista de Contorno + Gradientes')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. Convergencia del costo
    ax3 = fig.add_subplot(133)
    ax3.plot(range(len(cost_traj)), cost_traj, 'b-', linewidth=2)
    ax3.set_xlabel('Iteraci√≥n')
    ax3.set_ylabel('Costo MSE')
    ax3.set_title('Convergencia del Costo')
    ax3.grid(True, alpha=0.3)
    ax3.semilogy()  # Escala logar√≠tmica

    plt.tight_layout()
    plt.show()

    print(f"Punto inicial: w={w_traj[0]:.3f}, b={b_traj[0]:.3f}")
    print(f"Punto final:   w={w_traj[-1]:.3f}, b={b_traj[-1]:.3f}")
    print(f"Iteraciones:   {len(cost_traj)}")

visualizar_landscape_3d()
```

---

## Parte 6: Proyecto Semanal - Gradient Descent desde Cero

### El Desaf√≠o

Tu misi√≥n esta semana es implementar y validar un algoritmo completo de gradient descent para regresi√≥n lineal multivariable, comparando diferentes configuraciones y analizando su comportamiento.

```python
class GradientDescentProyecto:
    """
    Implementaci√≥n completa para el proyecto semanal
    Incluye m√∫ltiples variantes y an√°lisis exhaustivo
    """

    def __init__(self, learning_rate=0.01, max_iter=1000, tolerance=1e-6,
                 regularization=None, lambda_reg=0.01):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.tolerance = tolerance
        self.regularization = regularization  # None, 'l1', 'l2'
        self.lambda_reg = lambda_reg
        self.reset_history()

    def reset_history(self):
        """Reiniciar historial de entrenamiento"""
        self.history = {
            'cost': [],
            'cost_regularized': [],
            'theta': [],
            'gradients': [],
            'gradient_norms': [],
            'learning_rates': []  # Para learning rate adaptativo
        }

    def _add_intercept(self, X):
        """Agregar columna de bias"""
        return np.column_stack([np.ones(X.shape[0]), X])

    def _cost_function(self, X, y, theta, return_regularized=False):
        """
        Funci√≥n de costo con regularizaci√≥n opcional
        """
        m = len(y)
        h = X.dot(theta)
        cost = (1/(2*m)) * np.sum((h - y)**2)

        # Agregar regularizaci√≥n si est√° especificada
        reg_cost = 0
        if self.regularization == 'l2':
            # L2 regularization (Ridge)
            reg_cost = self.lambda_reg * np.sum(theta[1:]**2)  # No regularizar bias
        elif self.regularization == 'l1':
            # L1 regularization (Lasso)
            reg_cost = self.lambda_reg * np.sum(np.abs(theta[1:]))

        if return_regularized:
            return cost, cost + reg_cost
        return cost + reg_cost

    def _gradient(self, X, y, theta):
        """
        Calcular gradiente con regularizaci√≥n opcional
        """
        m = len(y)
        h = X.dot(theta)
        gradient = (1/m) * X.T.dot(h - y)

        # Agregar t√©rmino de regularizaci√≥n al gradiente
        if self.regularization == 'l2':
            # Para L2: agregar Œª*Œ∏ (excepto bias)
            reg_gradient = np.zeros_like(theta)
            reg_gradient[1:] = self.lambda_reg * theta[1:]
            gradient += reg_gradient
        elif self.regularization == 'l1':
            # Para L1: agregar Œª*sign(Œ∏) (excepto bias)
            reg_gradient = np.zeros_like(theta)
            reg_gradient[1:] = self.lambda_reg * np.sign(theta[1:])
            gradient += reg_gradient

        return gradient

    def fit(self, X, y, adaptive_lr=False, verbose=True):
        """
        Entrenar con opciones avanzadas
        """
        X = self._add_intercept(X)
        self.reset_history()

        # Inicializaci√≥n inteligente de par√°metros
        self.theta = np.random.normal(0, 0.01, X.shape[1])

        # Para learning rate adaptativo
        lr_current = self.learning_rate
        cost_anterior = float('inf')
        paciencia_lr = 0

        if verbose:
            print(f"üöÄ Iniciando entrenamiento...")
            print(f"Datos: {X.shape[0]} muestras, {X.shape[1]-1} caracter√≠sticas")
            print(f"Regularizaci√≥n: {self.regularization}")
            print(f"Learning rate adaptativo: {adaptive_lr}")
            print("-" * 60)

        for iteration in range(self.max_iter):
            # Forward pass
            cost_base, cost_reg = self._cost_function(X, y, self.theta, True)

            # Backward pass
            gradient = self._gradient(X, y, self.theta)
            gradient_norm = np.linalg.norm(gradient)

            # Actualizaci√≥n de par√°metros
            theta_anterior = self.theta.copy()
            self.theta -= lr_current * gradient

            # Learning rate adaptativo
            if adaptive_lr and iteration > 10:
                if cost_reg > cost_anterior:
                    # Si el costo aument√≥, reducir learning rate
                    lr_current *= 0.9
                    paciencia_lr += 1
                    if paciencia_lr > 5:
                        if verbose:
                            print(f"Iteraci√≥n {iteration}: Reduciendo LR a {lr_current:.6f}")
                        paciencia_lr = 0
                else:
                    # Si el costo disminuy√≥, aumentar ligeramente el learning rate
                    lr_current = min(lr_current * 1.01, self.learning_rate * 2)

            # Guardar historial
            self.history['cost'].append(cost_base)
            self.history['cost_regularized'].append(cost_reg)
            self.history['theta'].append(self.theta.copy())
            self.history['gradients'].append(gradient.copy())
            self.history['gradient_norms'].append(gradient_norm)
            self.history['learning_rates'].append(lr_current)

            # Verificar convergencia
            if gradient_norm < self.tolerance:
                if verbose:
                    print(f"‚úÖ Convergencia por gradiente en iteraci√≥n {iteration}")
                break

            if np.linalg.norm(self.theta - theta_anterior) < self.tolerance:
                if verbose:
                    print(f"‚úÖ Convergencia por par√°metros en iteraci√≥n {iteration}")
                break

            cost_anterior = cost_reg

            if verbose and iteration % 200 == 0:
                print(f"Iter {iteration:4d}: Costo = {cost_reg:.6f}, "
                      f"||‚àá|| = {gradient_norm:.6f}, LR = {lr_current:.6f}")

        self.n_iterations = len(self.history['cost'])

        if verbose:
            print(f"üèÅ Entrenamiento completado en {self.n_iterations} iteraciones")
            print(f"Costo final: {self.history['cost_regularized'][-1]:.6f}")
            print(f"Par√°metros finales: {self.theta}")

        return self

    def predict(self, X):
        """Predicciones"""
        X = self._add_intercept(X)
        return X.dot(self.theta)

    def score(self, X, y):
        """R¬≤ score"""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        return 1 - (ss_res / ss_tot)

    def plot_comprehensive_analysis(self):
        """
        An√°lisis visual completo del entrenamiento
        """
        fig = plt.figure(figsize=(20, 16))

        iterations = range(len(self.history['cost']))

        # 1. Convergencia de costos
        ax1 = plt.subplot(3, 3, 1)
        plt.plot(iterations, self.history['cost'], 'b-',
                label='Costo base (MSE)', linewidth=2)
        if self.regularization:
            plt.plot(iterations, self.history['cost_regularized'], 'r-',
                    label='Costo regularizado', linewidth=2)
        plt.yscale('log')
        plt.xlabel('Iteraci√≥n')
        plt.ylabel('Costo (escala log)')
        plt.title('1. Convergencia del Costo')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 2. Evoluci√≥n de par√°metros
        ax2 = plt.subplot(3, 3, 2)
        theta_history = np.array(self.history['theta'])
        for i in range(min(5, theta_history.shape[1])):  # Max 5 par√°metros para claridad
            label = 'Bias' if i == 0 else f'Œ∏_{i}'
            plt.plot(iterations, theta_history[:, i], label=label, linewidth=2)
        plt.xlabel('Iteraci√≥n')
        plt.ylabel('Valor del Par√°metro')
        plt.title('2. Evoluci√≥n de Par√°metros')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 3. Norma del gradiente
        ax3 = plt.subplot(3, 3, 3)
        plt.plot(iterations, self.history['gradient_norms'], 'g-', linewidth=2)
        plt.yscale('log')
        plt.xlabel('Iteraci√≥n')
        plt.ylabel('||‚àáJ|| (escala log)')
        plt.title('3. Magnitud del Gradiente')
        plt.grid(True, alpha=0.3)

        # 4. Learning rate adaptativo
        ax4 = plt.subplot(3, 3, 4)
        plt.plot(iterations, self.history['learning_rates'], 'orange', linewidth=2)
        plt.xlabel('Iteraci√≥n')
        plt.ylabel('Learning Rate')
        plt.title('4. Learning Rate Adaptativo')
        plt.grid(True, alpha=0.3)

        # 5. Distribuci√≥n de gradientes por componente
        ax5 = plt.subplot(3, 3, 5)
        gradients = np.array(self.history['gradients'])
        # Tomar solo las √∫ltimas iteraciones para ver la convergencia
        recent_grads = gradients[-min(100, len(gradients)):]
        for i in range(min(3, gradients.shape[1])):
            label = '‚àá(Bias)' if i == 0 else f'‚àáŒ∏_{i}'
            plt.hist(recent_grads[:, i], bins=20, alpha=0.7, label=label)
        plt.xlabel('Valor del Gradiente')
        plt.ylabel('Frecuencia')
        plt.title('5. Distribuci√≥n de Gradientes (√∫ltimas iter.)')
        plt.legend()

        # 6. An√°lisis de la convergencia (cambios en par√°metros)
        ax6 = plt.subplot(3, 3, 6)
        cambios_theta = [0] + [np.linalg.norm(self.history['theta'][i] -
                                             self.history['theta'][i-1])
                               for i in range(1, len(self.history['theta']))]
        plt.plot(iterations, cambios_theta, 'purple', linewidth=2)
        plt.yscale('log')
        plt.xlabel('Iteraci√≥n')
        plt.ylabel('||ŒîŒ∏|| (escala log)')
        plt.title('6. Cambio en Par√°metros')
        plt.grid(True, alpha=0.3)

        # 7. Trajectory en espacio 2D de par√°metros (si hay exactamente 2 par√°metros)
        ax7 = plt.subplot(3, 3, 7)
        if theta_history.shape[1] >= 2:
            plt.plot(theta_history[:, 0], theta_history[:, 1], 'b-o',
                    markersize=2, linewidth=1, alpha=0.7)
            plt.plot(theta_history[0, 0], theta_history[0, 1], 'go',
                    markersize=10, label='Inicio')
            plt.plot(theta_history[-1, 0], theta_history[-1, 1], 'ro',
                    markersize=10, label='Final')
            plt.xlabel('Œ∏‚ÇÄ (Bias)')
            plt.ylabel('Œ∏‚ÇÅ')
            plt.title('7. Trayectoria en Espacio de Par√°metros')
            plt.legend()
        else:
            plt.text(0.5, 0.5, 'Necesita ‚â•2 par√°metros', ha='center', va='center')
            plt.title('7. Trayectoria (N/A)')
        plt.grid(True, alpha=0.3)

        # 8. Rate de convergencia (log del costo vs iteraci√≥n)
        ax8 = plt.subplot(3, 3, 8)
        log_cost = np.log(self.history['cost_regularized'])
        if len(log_cost) > 10:
            # Ajustar l√≠nea recta a las √∫ltimas iteraciones para estimar rate
            x_fit = np.arange(len(log_cost)//2, len(log_cost))
            y_fit = log_cost[len(log_cost)//2:]
            if len(x_fit) > 1:
                slope, intercept = np.polyfit(x_fit, y_fit, 1)
                plt.plot(iterations, log_cost, 'b-', linewidth=2, label='log(Costo)')
                plt.plot(x_fit, slope*x_fit + intercept, 'r--', linewidth=2,
                        label=f'Pendiente ‚âà {slope:.4f}')
                plt.legend()
        plt.xlabel('Iteraci√≥n')
        plt.ylabel('log(Costo)')
        plt.title('8. Rate de Convergencia')
        plt.grid(True, alpha=0.3)

        # 9. Estad√≠sticas finales
        ax9 = plt.subplot(3, 3, 9)
        ax9.axis('off')
        stats_text = f"""
        ESTAD√çSTICAS FINALES

        Iteraciones: {self.n_iterations}
        Costo final: {self.history['cost_regularized'][-1]:.6f}
        ||‚àá|| final: {self.history['gradient_norms'][-1]:.2e}

        Par√°metros finales:
        """

        for i, theta in enumerate(self.theta):
            param_name = 'Bias' if i == 0 else f'Œ∏_{i}'
            stats_text += f"  {param_name}: {theta:.4f}\n"

        if self.regularization:
            stats_text += f"\nRegularizaci√≥n: {self.regularization}"
            stats_text += f"\nŒª = {self.lambda_reg}"

        ax9.text(0.1, 0.9, stats_text, transform=ax9.transAxes,
                fontsize=10, verticalalignment='top', fontfamily='monospace')

        plt.tight_layout()
        plt.show()

# Funci√≥n para ejecutar experimentos comparativos
def experimentos_comparativos():
    """
    Ejecutar m√∫ltiples experimentos para comparar diferentes configuraciones
    """
    print("üß™ EXPERIMENTOS COMPARATIVOS")
    print("=" * 80)

    # Generar dataset m√°s complejo
    np.random.seed(42)
    n_samples = 300
    n_features = 4

    # Features correlacionadas para hacer el problema m√°s interesante
    X = np.random.randn(n_samples, n_features)
    X[:, 1] += 0.5 * X[:, 0]  # Correlaci√≥n entre features

    true_theta = np.array([0.5, 2.0, -1.5, 0.8, 0.3])  # [bias, w1, w2, w3, w4]
    noise_level = 0.1

    X_with_bias = np.column_stack([np.ones(n_samples), X])
    y = X_with_bias @ true_theta + noise_level * np.random.randn(n_samples)

    print(f"Dataset: {n_samples} muestras, {n_features} caracter√≠sticas")
    print(f"Par√°metros verdaderos: {true_theta}")
    print(f"Nivel de ruido: {noise_level}")
    print()

    # Dividir en train/test
    split_idx = int(0.8 * n_samples)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # Experimentos con diferentes configuraciones
    configuraciones = [
        {'name': 'SGD B√°sico', 'lr': 0.01, 'reg': None},
        {'name': 'SGD LR Alto', 'lr': 0.1, 'reg': None},
        {'name': 'SGD + L2', 'lr': 0.01, 'reg': 'l2', 'lambda': 0.01},
        {'name': 'SGD + L1', 'lr': 0.01, 'reg': 'l1', 'lambda': 0.01},
        {'name': 'SGD Adaptativo', 'lr': 0.05, 'reg': None, 'adaptive': True},
    ]

    resultados = []

    for config in configuraciones:
        print(f"üîÑ Ejecutando: {config['name']}")

        modelo = GradientDescentProyecto(
            learning_rate=config['lr'],
            max_iter=2000,
            regularization=config.get('reg'),
            lambda_reg=config.get('lambda', 0.01)
        )

        adaptive_lr = config.get('adaptive', False)
        modelo.fit(X_train, y_train, adaptive_lr=adaptive_lr, verbose=False)

        # Evaluaci√≥n
        train_score = modelo.score(X_train, y_train)
        test_score = modelo.score(X_test, y_test)

        # Error en par√°metros
        param_error = np.linalg.norm(modelo.theta - true_theta)

        resultado = {
            'config': config['name'],
            'train_r2': train_score,
            'test_r2': test_score,
            'param_error': param_error,
            'iterations': modelo.n_iterations,
            'final_cost': modelo.history['cost_regularized'][-1],
            'modelo': modelo
        }

        resultados.append(resultado)
        print(f"  ‚úÖ R¬≤ train: {train_score:.4f}, R¬≤ test: {test_score:.4f}")

    print()
    print("üìä RESUMEN DE RESULTADOS:")
    print("-" * 80)
    print(f"{'Configuraci√≥n':<15} {'R¬≤ Train':<10} {'R¬≤ Test':<10} {'Error Param':<12} {'Iter':<6}")
    print("-" * 80)

    for r in resultados:
        print(f"{r['config']:<15} {r['train_r2']:<10.4f} {r['test_r2']:<10.4f} "
              f"{r['param_error']:<12.4f} {r['iterations']:<6d}")

    # Visualizaci√≥n comparativa
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    # 1. Comparaci√≥n de convergencia
    for r in resultados:
        iterations = range(len(r['modelo'].history['cost_regularized']))
        ax1.plot(iterations, r['modelo'].history['cost_regularized'],
                label=r['config'], linewidth=2)
    ax1.set_yscale('log')
    ax1.set_xlabel('Iteraci√≥n')
    ax1.set_ylabel('Costo (escala log)')
    ax1.set_title('Convergencia de Diferentes Configuraciones')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. R¬≤ comparaci√≥n
    configs = [r['config'] for r in resultados]
    train_scores = [r['train_r2'] for r in resultados]
    test_scores = [r['test_r2'] for r in resultados]

    x_pos = np.arange(len(configs))
    ax2.bar(x_pos - 0.2, train_scores, 0.4, label='Train', alpha=0.8)
    ax2.bar(x_pos + 0.2, test_scores, 0.4, label='Test', alpha=0.8)
    ax2.set_xlabel('Configuraci√≥n')
    ax2.set_ylabel('R¬≤')
    ax2.set_title('Comparaci√≥n de Performance')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(configs, rotation=45)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. Error en par√°metros
    param_errors = [r['param_error'] for r in resultados]
    ax3.bar(configs, param_errors, alpha=0.8, color='orange')
    ax3.set_xlabel('Configuraci√≥n')
    ax3.set_ylabel('||Œ∏_estimado - Œ∏_verdadero||')
    ax3.set_title('Error en Estimaci√≥n de Par√°metros')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(True, alpha=0.3)

    # 4. N√∫mero de iteraciones
    iterations_list = [r['iterations'] for r in resultados]
    ax4.bar(configs, iterations_list, alpha=0.8, color='green')
    ax4.set_xlabel('Configuraci√≥n')
    ax4.set_ylabel('N√∫mero de Iteraciones')
    ax4.set_title('Velocidad de Convergencia')
    ax4.tick_params(axis='x', rotation=45)
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return resultados

# Ejecutar experimentos
resultados_exp = experimentos_comparativos()

# An√°lisis detallado del mejor modelo
print("\nüèÜ AN√ÅLISIS DETALLADO DEL MEJOR MODELO")
print("=" * 50)

mejor_modelo_idx = max(range(len(resultados_exp)),
                      key=lambda i: resultados_exp[i]['test_r2'])
mejor_modelo = resultados_exp[mejor_modelo_idx]['modelo']
mejor_config = resultados_exp[mejor_modelo_idx]['config']

print(f"Mejor configuraci√≥n: {mejor_config}")
mejor_modelo.plot_comprehensive_analysis()
```

### Desaf√≠os Adicionales

Para llevar tu comprensi√≥n al siguiente nivel, intenta estos desaf√≠os:

```python
def desafios_avanzados():
    """
    Desaf√≠os adicionales para profundizar el entendimiento
    """
    print("üéØ DESAF√çOS AVANZADOS")
    print("=" * 50)

    print("""
    1. üßÆ IMPLEMENTA DIFERENTES OPTIMIZADORES:
       - SGD con Momentum
       - RMSprop (simplificado)
       - Adam (simplificado)

    2. üîç GRADIENT CHECKING:
       - Implementa verificaci√≥n num√©rica de gradientes
       - Compara con gradientes anal√≠ticos
       - Encuentra bugs en implementaciones

    3. üìä AN√ÅLISIS DE SENSIBILIDAD:
       - ¬øC√≥mo afecta el ruido en los datos?
       - ¬øQu√© pasa con datos no lineales?
       - ¬øC√≥mo se comporta con outliers?

    4. üéõÔ∏è HYPERPARAMETER TUNING:
       - Grid search para learning rate √≥ptimo
       - Regularizaci√≥n autom√°tica
       - Early stopping

    5. üî¢ BATCH PROCESSING:
       - Mini-batch gradient descent
       - Stochastic gradient descent
       - Comparaci√≥n de varianza vs velocidad
    """)

# Ejemplo de implementaci√≥n de SGD con Momentum
class SGDMomentum:
    """
    Implementaci√≥n de SGD con Momentum como ejemplo
    """

    def __init__(self, learning_rate=0.01, momentum=0.9, max_iter=1000):
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.max_iter = max_iter

    def fit(self, X, y):
        X = np.column_stack([np.ones(X.shape[0]), X])
        self.theta = np.random.normal(0, 0.01, X.shape[1])
        self.velocity = np.zeros_like(self.theta)

        self.history = {'cost': [], 'theta': []}

        for i in range(self.max_iter):
            # Forward pass
            h = X.dot(self.theta)
            cost = np.mean((h - y)**2)

            # Backward pass
            gradient = (1/len(y)) * X.T.dot(h - y)

            # Update con momentum
            self.velocity = self.momentum * self.velocity - self.learning_rate * gradient
            self.theta += self.velocity

            self.history['cost'].append(cost)
            self.history['theta'].append(self.theta.copy())

        return self

    def predict(self, X):
        X = np.column_stack([np.ones(X.shape[0]), X])
        return X.dot(self.theta)

# Demostraci√≥n r√°pida
def demo_momentum():
    """Demo r√°pida de SGD con momentum"""
    np.random.seed(42)
    X = np.random.randn(100, 2)
    y = 2*X[:,0] - X[:,1] + 1 + 0.1*np.random.randn(100)

    # SGD cl√°sico vs SGD con momentum
    sgd_clasico = GradientDescentProyecto(learning_rate=0.1, max_iter=500)
    sgd_momentum = SGDMomentum(learning_rate=0.1, momentum=0.9, max_iter=500)

    sgd_clasico.fit(X, y, verbose=False)
    sgd_momentum.fit(X, y)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(sgd_clasico.history['cost_regularized'], 'b-',
             label='SGD Cl√°sico', linewidth=2)
    plt.plot(sgd_momentum.history['cost'], 'r-',
             label='SGD + Momentum', linewidth=2)
    plt.yscale('log')
    plt.xlabel('Iteraci√≥n')
    plt.ylabel('Costo (escala log)')
    plt.title('SGD vs SGD + Momentum')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    theta_sgd = np.array(sgd_clasico.history['theta'])
    theta_mom = np.array(sgd_momentum.history['theta'])

    plt.plot(theta_sgd[:, 0], theta_sgd[:, 1], 'b-', label='SGD Cl√°sico', alpha=0.7)
    plt.plot(theta_mom[:, 0], theta_mom[:, 1], 'r-', label='SGD + Momentum', alpha=0.7)
    plt.xlabel('Œ∏‚ÇÄ (bias)')
    plt.ylabel('Œ∏‚ÇÅ')
    plt.title('Trayectorias en Espacio de Par√°metros')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

desafios_avanzados()
demo_momentum()
```

---

## Parte 7: Conexi√≥n con el Futuro - ¬øQu√© Viene Despu√©s?

### El Puente hacia Deep Learning

Todo lo que aprendiste esta semana es la **base fundamental** de deep learning:

```python
def preview_deep_learning():
    """
    Vista previa de c√≥mo se conecta con deep learning
    """
    print("üîÆ CONEXI√ìN CON DEEP LEARNING")
    print("=" * 50)

    print("""
    üß† REDES NEURONALES:
    - Cada neurona aplica: output = œÉ(w¬∑x + b)
    - œÉ es una funci√≥n de activaci√≥n (sigmoid, ReLU, etc.)
    - La derivada de œÉ es crucial para backpropagation

    üîó BACKPROPAGATION:
    - Es simplemente la regla de la cadena aplicada repetidamente
    - ‚àÇLoss/‚àÇw = ‚àÇLoss/‚àÇoutput √ó ‚àÇoutput/‚àÇw
    - Cada capa propaga el gradiente hacia atr√°s

    ‚ö° OPTIMIZADORES AVANZADOS:
    - Adam: combina momentum + RMSprop
    - AdaGrad: learning rates adaptativos por par√°metro
    - Todos usan los mismos principios de gradient descent

    üìä FUNCIONES DE P√âRDIDA:
    - Cross-entropy para clasificaci√≥n
    - Huber loss para robustez a outliers
    - Todas se optimizan con gradientes
    """)

    # Ejemplo simple de "neurona" artificial
    def neurona_sigmoid(x, w, b):
        z = np.dot(w, x) + b
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Sigmoid

    def derivada_sigmoid(a):
        return a * (1 - a)

    # Visualizaci√≥n de una "mini red neuronal"
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # 1. Funci√≥n de activaci√≥n y su derivada
    z = np.linspace(-6, 6, 1000)
    sigmoid_vals = 1 / (1 + np.exp(-z))
    sigmoid_deriv_vals = sigmoid_vals * (1 - sigmoid_vals)

    axes[0].plot(z, sigmoid_vals, 'b-', linewidth=2, label='œÉ(z)')
    axes[0].plot(z, sigmoid_deriv_vals, 'r--', linewidth=2, label="œÉ'(z)")
    axes[0].set_xlabel('z = wx + b')
    axes[0].set_ylabel('Activaci√≥n')
    axes[0].set_title('Neurona: Activaci√≥n y Derivada')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # 2. Superficie de p√©rdida para una neurona simple
    w_range = np.linspace(-3, 3, 50)
    b_range = np.linspace(-3, 3, 50)
    W, B = np.meshgrid(w_range, b_range)

    # Datos de ejemplo para clasificaci√≥n binaria
    np.random.seed(42)
    X_class = np.random.randn(50, 1)
    y_class = (X_class > 0).astype(float).flatten()

    # Cross-entropy loss para diferentes w, b
    def cross_entropy_loss(w, b):
        predictions = neurona_sigmoid(X_class.flatten(), w, b)
        predictions = np.clip(predictions, 1e-7, 1-1e-7)  # Evitar log(0)
        return -np.mean(y_class * np.log(predictions) + (1-y_class) * np.log(1-predictions))

    Loss = np.array([[cross_entropy_loss(w, b) for w in w_range] for b in b_range])

    contour = axes[1].contour(W, B, Loss, levels=20)
    axes[1].clabel(contour, inline=True, fontsize=8)
    axes[1].set_xlabel('Peso (w)')
    axes[1].set_ylabel('Sesgo (b)')
    axes[1].set_title('Landscape de Cross-Entropy Loss')
    axes[1].grid(True, alpha=0.3)

    # 3. Comparaci√≥n de funciones de p√©rdida
    y_true = 1  # Clase verdadera
    predictions = np.linspace(0.01, 0.99, 100)

    mse_loss = (predictions - y_true)**2
    cross_entropy = -np.log(predictions)  # Para y_true = 1

    axes[2].plot(predictions, mse_loss, 'b-', linewidth=2, label='MSE Loss')
    axes[2].plot(predictions, cross_entropy, 'r-', linewidth=2, label='Cross-Entropy')
    axes[2].set_xlabel('Predicci√≥n')
    axes[2].set_ylabel('P√©rdida')
    axes[2].set_title('Comparaci√≥n de Funciones de P√©rdida')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

preview_deep_learning()
```

### Prepar√°ndote para la Semana 4

La pr√≥xima semana nos sumergiremos en **estad√≠stica y probabilidad**, que son el otro pilar fundamental del machine learning:

```python
def preview_semana_4():
    """
    Vista previa de lo que viene en Estad√≠stica y Probabilidad
    """
    print("üîú PR√ìXIMA SEMANA: ESTAD√çSTICA Y PROBABILIDAD")
    print("=" * 60)

    print("""
    üìà DISTRIBUCIONES DE PROBABILIDAD:
    - ¬øPor qu√© los algoritmos de ML hacen "suposiciones" sobre los datos?
    - Distribuci√≥n normal, Bernoulli, Poisson
    - M√°xima verosimilitud: la conexi√≥n con funciones de costo

    üéØ INFERENCIA ESTAD√çSTICA:
    - Intervalos de confianza para predicciones
    - Tests de hip√≥tesis para validar modelos
    - Bootstrap y validaci√≥n cruzada

    üé≤ PROBABILIDAD BAYESIANA:
    - Naive Bayes: probabilidad aplicada a clasificaci√≥n
    - Prior, likelihood, posterior: el tr√≠o m√°gico
    - Incertidumbre en machine learning

    üìä CONEXIONES CON HOY:
    - MLE (Maximum Likelihood) ‚Üí minimizar p√©rdida ‚Üí gradient descent
    - Regularizaci√≥n ‚Üí distribuciones prior bayesianas
    - Cross-validation ‚Üí distribuciones muestrales
    """)

    # Ejemplo simple: conexi√≥n MLE con gradient descent
    print("\nüîó CONEXI√ìN DIRECTA CON GRADIENT DESCENT:")
    print("-" * 40)

    print("Para regresi√≥n lineal con ruido gaussiano:")
    print("  1. Suponemos: y = Xw + Œµ, donde Œµ ~ N(0, œÉ¬≤)")
    print("  2. Maximum Likelihood Estimation:")
    print("     L(w) = ‚àè P(y·µ¢ | x·µ¢, w)")
    print("  3. Log-likelihood: log L(w) = -¬Ω‚àë(y·µ¢ - x·µ¢w)¬≤/œÉ¬≤")
    print("  4. Maximizar log L(w) ‚â° Minimizar ‚àë(y·µ¢ - x·µ¢w)¬≤")
    print("  5. ¬°Eso es exactamente MSE que optimizamos con gradient descent!")

    # Visualizaci√≥n r√°pida
    plt.figure(figsize=(15, 5))

    # 1. Distribuci√≥n normal del error
    plt.subplot(1, 3, 1)
    x_error = np.linspace(-3, 3, 1000)
    y_normal = (1/np.sqrt(2*np.pi)) * np.exp(-0.5 * x_error**2)
    plt.plot(x_error, y_normal, 'b-', linewidth=2)
    plt.fill_between(x_error, 0, y_normal, alpha=0.3)
    plt.axvline(0, color='red', linestyle='--', label='Error = 0')
    plt.xlabel('Error (Œµ)')
    plt.ylabel('Densidad de Probabilidad')
    plt.title('Suposici√≥n: Errores ~ N(0,œÉ¬≤)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 2. Funci√≥n de verosimilitud vs MSE
    plt.subplot(1, 3, 2)
    w_vals = np.linspace(-2, 4, 100)

    # Para datos sint√©ticos simples
    np.random.seed(42)
    x_simple = np.array([1, 2, 3, 4, 5])
    y_simple = 2 * x_simple + 1 + 0.2 * np.random.randn(5)

    mse_vals = [(np.sum((y_simple - w * x_simple)**2)) for w in w_vals]
    log_likelihood = [-0.5 * mse for mse in mse_vals]  # Simplificado

    plt.plot(w_vals, mse_vals, 'r-', linewidth=2, label='MSE')
    plt.xlabel('Par√°metro w')
    plt.ylabel('MSE')
    plt.title('MSE a Minimizar')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 3. Log-likelihood (a maximizar)
    plt.subplot(1, 3, 3)
    plt.plot(w_vals, log_likelihood, 'g-', linewidth=2, label='Log-Likelihood')
    plt.axvline(w_vals[np.argmax(log_likelihood)], color='red', linestyle='--',
                label='MLE √ìptimo')
    plt.xlabel('Par√°metro w')
    plt.ylabel('Log-Likelihood')
    plt.title('Log-Likelihood a Maximizar')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    print("\nüí° LA GRAN REVELACI√ìN:")
    print("Gradient descent no es solo un 'truco de optimizaci√≥n'.")
    print("Es la implementaci√≥n computacional de principios estad√≠sticos profundos!")

preview_semana_4()
```

---

## Resumen y Pr√≥ximos Pasos

### Lo que Dominaste Esta Semana

```python
def resumen_semanal():
    """
    Resumen completo de los conceptos aprendidos
    """
    print("üéâ RESUMEN DE LA SEMANA 3")
    print("=" * 50)

    print("‚úÖ CONCEPTOS DOMINADOS:")
    print("""
    üî¢ DERIVADAS:
    - Intuici√≥n geom√©trica como pendiente de la tangente
    - Reglas b√°sicas: potencia, suma, producto, cadena
    - Implementaci√≥n num√©rica vs anal√≠tica

    üåê DERIVADAS PARCIALES:
    - Funciones de m√∫ltiples variables
    - Vector gradiente como direcci√≥n de m√°ximo crecimiento
    - Interpretaci√≥n geom√©trica en 3D

    ‚ö° GRADIENT DESCENT:
    - Algoritmo de optimizaci√≥n fundamental
    - Importancia del learning rate
    - Convergencia y problemas comunes

    üõ†Ô∏è IMPLEMENTACI√ìN PR√ÅCTICA:
    - Gradient descent desde cero
    - Diferentes variantes y optimizadores
    - An√°lisis de convergencia y debugging

    ü§ñ CONEXI√ìN CON ML:
    - Funciones de costo comunes (MSE, cross-entropy)
    - Regularizaci√≥n L1 y L2
    - Fundamentos para redes neuronales
    """)

    print("\nüéØ HABILIDADES PR√ÅCTICAS ADQUIRIDAS:")
    skills = [
        "Implementar gradient descent desde cero",
        "Calcular gradientes num√©ricos para verificaci√≥n",
        "Visualizar paisajes de optimizaci√≥n en 2D/3D",
        "Diagnosticar problemas de convergencia",
        "Comparar diferentes optimizadores",
        "Aplicar regularizaci√≥n para evitar overfitting",
        "Conectar teor√≠a matem√°tica con implementaci√≥n pr√°ctica"
    ]

    for i, skill in enumerate(skills, 1):
        print(f"  {i}. {skill}")

    print("\nüöÄ PREPARADO PARA:")
    print("  - Estad√≠stica y Probabilidad (Semana 4)")
    print("  - Algoritmos de Machine Learning supervisado")
    print("  - Redes neuronales y deep learning")
    print("  - Optimizaci√≥n avanzada en problemas reales")

resumen_semanal()
```

### Desaf√≠o Final de la Semana

```python
def desafio_final():
    """
    Desaf√≠o integrador para consolidar todo el aprendizaje
    """
    print("üèÜ DESAF√çO FINAL: OPTIMIZADOR INTELIGENTE")
    print("=" * 60)

    print("""
    üéØ TU MISI√ìN:
    Implementa un optimizador que combine TODAS las t√©cnicas aprendidas:

    1. üìä M√öLTIPLES ALGORITMOS:
       - SGD b√°sico
       - SGD con momentum
       - Learning rate adaptativo

    2. üõ°Ô∏è ROBUSTEZ:
       - Gradient clipping para gradientes explosivos
       - Early stopping para evitar overfitting
       - Regularizaci√≥n autom√°tica

    3. üìà MONITOREO:
       - M√©tricas de convergencia en tiempo real
       - Detecci√≥n autom√°tica de problemas
       - Visualizaciones interactivas

    4. üß™ VALIDACI√ìN:
       - Gradient checking autom√°tico
       - Comparaci√≥n con soluciones anal√≠ticas
       - Tests unitarios para cada componente

    üìù ENTREGABLES:
    - C√≥digo comentado y documentado
    - An√°lisis comparativo de performance
    - Visualizaciones comprehensivas
    - Reporte t√©cnico con conclusiones
    """)

    print("\nüí° CRITERIOS DE EVALUACI√ìN:")
    criterios = [
        "Correcci√≥n matem√°tica de las implementaciones",
        "Calidad del c√≥digo (legibilidad, documentaci√≥n)",
        "Profundidad del an√°lisis experimental",
        "Creatividad en las visualizaciones",
        "Conexi√≥n con conceptos te√≥ricos",
        "Preparaci√≥n para temas avanzados"
    ]

    for i, criterio in enumerate(criterios, 1):
        print(f"  {i}. {criterio}")

    print(f"\n‚è∞ TIEMPO ESTIMADO: 4-6 horas")
    print(f"üéÅ RECOMPENSA: Comprensi√≥n profunda del motor de todo ML moderno")

desafio_final()
```

---

## Palabras Finales

Felicitaciones por completar la semana m√°s matem√°ticamente intensa del programa. Lo que acab√°s de aprender no es solo teor√≠a abstracta: **es el coraz√≥n pulsante de toda la inteligencia artificial moderna**.

Cada vez que una red neuronal aprende a reconocer im√°genes, cada vez que un modelo de lenguaje genera texto coherente, cada vez que un algoritmo de recomendaci√≥n sugiere contenido personalizado, **est√° usando los principios de c√°lculo diferencial que dominaste hoy**.

El gradient descent que implementaste desde cero es el mismo algoritmo (con variaciones) que entrena:
- GPT y otros modelos de lenguaje
- Redes convolucionales para visi√≥n computacional
- Sistemas de recomendaci√≥n de Netflix y Spotify
- Algoritmos de trading automatizado
- Modelos de predicci√≥n m√©dica

**Has construido los cimientos. Ahora viene lo divertido: construir el edificio.**

La pr√≥xima semana, cuando exploremos estad√≠stica y probabilidad, vas a ver c√≥mo estos conceptos matem√°ticos se conectan con la **incertidumbre** y la **toma de decisiones** - los otros pilares fundamentales de la inteligencia artificial.

Pero por ahora, tomate un momento para apreciar lo que lograste. Pasaste de no entender qu√© era una derivada a implementar algoritmos de optimizaci√≥n desde cero. **Eso no es poca cosa.**

**¬°Nos vemos la pr√≥xima semana para conquistar el mundo de la probabilidad!** üöÄ

---

{{< alert "circle-info" >}}
**Recursos adicionales para profundizar:**
- Khan Academy: Calculus
- 3Blue1Brown: Essence of Calculus (YouTube)
- MIT 18.01: Single Variable Calculus
- Coursera: Mathematics for Machine Learning Specialization
{{< /alert >}}
